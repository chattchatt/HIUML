{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b6f51d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-21T06:26:27.383136Z",
     "iopub.status.busy": "2025-11-21T06:26:27.382903Z",
     "iopub.status.idle": "2025-11-21T06:34:02.666796Z",
     "shell.execute_reply": "2025-11-21T06:34:02.665988Z"
    },
    "papermill": {
     "duration": 455.288726,
     "end_time": "2025-11-21T06:34:02.668095",
     "exception": false,
     "start_time": "2025-11-21T06:26:27.379369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found event files: ['event_01.csv', 'event_02.csv', 'event_03.csv', 'event_04.csv', 'event_05.csv', 'event_06.csv', 'event_07.csv', 'event_08.csv', 'event_09.csv', 'event_10.csv', 'event_11.csv', 'event_12.csv', 'event_13.csv', 'event_14.csv', 'event_15.csv', 'event_16.csv', 'event_17.csv']\n",
      "\n",
      "▶ Processing event_01.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_02.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_03.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_04.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_05.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_06.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_07.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_08.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_09.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_10.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_11.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_12.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_13.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_14.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_15.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_16.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing event_17.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shapes:\n",
      "X_lstm: (4184, 1500, 9)\n",
      "y: (4184,)\n",
      "\n",
      "Saved:\n",
      "- ./processed_features/X_lstm.npy\n",
      "- ./processed_features/y.npy\n",
      "- ./processed_features/transformer_features.parquet\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature generation for LSTM & Transformer models\n",
    "- Input  : /kaggle/input/machine-learning-basic/event_01.csv ~ event_17.csv\n",
    "- Output : \n",
    "    ./processed_features/X_lstm.npy               (N, seq_len, n_features)\n",
    "    ./processed_features/y.npy                   (N,)\n",
    "    ./processed_features/transformer_features.parquet  (N, n_tf_features + meta)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# ==================== 기본 세팅 ====================\n",
    "\n",
    "BASE_DIR = \"/kaggle/input/machine-learning-basic\"\n",
    "OUTPUT_DIR = \"./processed_features\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SEGMENT_SIZE = 150_000          # 세그먼트 길이\n",
    "DOWNSAMPLE_STEP = 100           # LSTM 시퀀스 다운샘플링 스텝 (150,000 -> 1,500)\n",
    "ROLLING_WINDOWS = (100, 1000, 10000)  # 롤링 윈도우 크기들 (샘플 단위)\n",
    "\n",
    "# ==================== 유틸 함수들 ====================\n",
    "\n",
    "def segment_iterator(signal: np.ndarray, ttf: np.ndarray, segment_size: int):\n",
    "    \"\"\"\n",
    "    1D 시계열을 segment_size 단위로 잘라서 반환하는 제너레이터.\n",
    "    마지막에 남는 자투리는 버린다.\n",
    "    \"\"\"\n",
    "    total_len = len(signal)\n",
    "    n_segments = total_len // segment_size\n",
    "    for i in range(n_segments):\n",
    "        start = i * segment_size\n",
    "        end = start + segment_size\n",
    "        seg_sig = signal[start:end]\n",
    "        seg_ttf = ttf[start:end]\n",
    "        yield i, seg_sig, seg_ttf\n",
    "\n",
    "\n",
    "def build_lstm_sequence(seg_signal: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    하나의 세그먼트(길이 150,000)의 원신호에서\n",
    "    LSTM용 시퀀스 피처 (1500, n_features)를 생성한다.\n",
    "    - 원신호 다운샘플\n",
    "    - 롤링 mean/std (다양한 window)\n",
    "    - zero crossing 시퀀스\n",
    "    - peak 시퀀스\n",
    "    \"\"\"\n",
    "    # pandas Series로 감싸서 rolling, ewm 사용\n",
    "    s = pd.Series(seg_signal.astype(np.float32))\n",
    "    \n",
    "    # 다운샘플 인덱스 (0, 100, 200, ..., 149900)\n",
    "    ds_idx = np.arange(0, len(s), DOWNSAMPLE_STEP)\n",
    "    \n",
    "    # 1) 원신호 다운샘플\n",
    "    raw_down = s.iloc[ds_idx].values.reshape(-1, 1)\n",
    "\n",
    "    # 2) 롤링 mean/std\n",
    "    rolling_feats = []\n",
    "    for w in ROLLING_WINDOWS:\n",
    "        roll_mean = (\n",
    "            s.rolling(window=w, min_periods=1)\n",
    "             .mean()\n",
    "             .iloc[ds_idx]\n",
    "             .values\n",
    "             .astype(np.float32)\n",
    "        )\n",
    "        roll_std = (\n",
    "            s.rolling(window=w, min_periods=1)\n",
    "             .std(ddof=0)\n",
    "             .fillna(0)\n",
    "             .iloc[ds_idx]\n",
    "             .values\n",
    "             .astype(np.float32)\n",
    "        )\n",
    "        rolling_feats.append(roll_mean.reshape(-1, 1))\n",
    "        rolling_feats.append(roll_std.reshape(-1, 1))\n",
    "\n",
    "    # 3) zero crossing 시퀀스 (부호가 달라지는 지점)\n",
    "    sign = np.sign(seg_signal)\n",
    "    sign[sign == 0] = 1  # 0은 +1로 처리\n",
    "    zc = np.zeros_like(seg_signal, dtype=np.float32)\n",
    "    zc[1:] = (sign[1:] != sign[:-1]).astype(np.float32)\n",
    "    zc_down = zc[ds_idx].reshape(-1, 1)\n",
    "\n",
    "    # 4) peak 시퀀스 (양옆보다 큰 지점)\n",
    "    # 간단한 1차 peak 탐지 (경계는 0으로)\n",
    "    peaks = np.zeros_like(seg_signal, dtype=np.float32)\n",
    "    mid = np.arange(1, len(seg_signal) - 1)\n",
    "    cond = (seg_signal[mid] > seg_signal[mid - 1]) & (seg_signal[mid] > seg_signal[mid + 1])\n",
    "    peaks[mid[cond]] = 1.0\n",
    "    peaks_down = peaks[ds_idx].reshape(-1, 1)\n",
    "\n",
    "    # 모든 피처 concat\n",
    "    seq_features = np.concatenate(\n",
    "        [raw_down] + rolling_feats + [zc_down, peaks_down],\n",
    "        axis=1\n",
    "    )\n",
    "    # shape: (seq_len=1500, n_features)\n",
    "    return seq_features.astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_spectral_features(seg_signal: np.ndarray, eps: float = 1e-12) -> dict:\n",
    "    \"\"\"\n",
    "    FFT 기반 spectral 피처 계산 (Transformer용 전역 피처 일부)\n",
    "    \"\"\"\n",
    "    # DC 제거를 위해 평균 0으로 정규화\n",
    "    x = seg_signal.astype(np.float32)\n",
    "    x = x - np.mean(x)\n",
    "\n",
    "    # rFFT\n",
    "    fft_vals = np.fft.rfft(x)\n",
    "    mag = np.abs(fft_vals)\n",
    "    power = mag ** 2\n",
    "\n",
    "    # 주파수 축 (정규화된 인덱스 기반)\n",
    "    freqs = np.fft.rfftfreq(len(x), d=1.0)\n",
    "\n",
    "    # 총 에너지\n",
    "    total_power = np.sum(power) + eps\n",
    "\n",
    "    # Spectral centroid\n",
    "    spectral_centroid = float(np.sum(freqs * power) / total_power)\n",
    "\n",
    "    # Spectral rolloff (총 에너지의 85%가 되는 지점)\n",
    "    cumulative_power = np.cumsum(power)\n",
    "    rolloff_threshold = 0.85 * total_power\n",
    "    rolloff_idx = np.searchsorted(cumulative_power, rolloff_threshold)\n",
    "    spectral_rolloff = float(freqs[min(rolloff_idx, len(freqs) - 1)])\n",
    "\n",
    "    # 몇 개 대역의 에너지 비율 (저/중/고)\n",
    "    # 전체 구간을 3분할\n",
    "    thirds = len(power) // 3\n",
    "    if thirds > 0:\n",
    "        low = float(np.sum(power[:thirds]) / total_power)\n",
    "        mid = float(np.sum(power[thirds:2*thirds]) / total_power)\n",
    "        high = float(np.sum(power[2*thirds:]) / total_power)\n",
    "    else:\n",
    "        low = mid = high = 0.0\n",
    "\n",
    "    return {\n",
    "        \"spec_total_power\": float(total_power),\n",
    "        \"spec_centroid\": spectral_centroid,\n",
    "        \"spec_rolloff\": spectral_rolloff,\n",
    "        \"spec_band_low\": low,\n",
    "        \"spec_band_mid\": mid,\n",
    "        \"spec_band_high\": high,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_transformer_features(seg_signal: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    하나의 세그먼트(150,000)에 대해 Transformer용 전역(summary) 피처 계산.\n",
    "    - 기본 통계\n",
    "    - 신호 기반 전체 지표\n",
    "    - FFT 기반 spectral 피처\n",
    "    - tsfresh 유사 전역 피처 몇 개\n",
    "    \"\"\"\n",
    "    x = seg_signal.astype(np.float32)\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # ----- 기본 통계 -----\n",
    "    feats[\"mean\"] = float(np.mean(x))\n",
    "    feats[\"std\"] = float(np.std(x))\n",
    "    feats[\"min\"] = float(np.min(x))\n",
    "    feats[\"max\"] = float(np.max(x))\n",
    "    feats[\"range\"] = float(feats[\"max\"] - feats[\"min\"])\n",
    "    feats[\"q25\"] = float(np.quantile(x, 0.25))\n",
    "    feats[\"q50\"] = float(np.quantile(x, 0.50))\n",
    "    feats[\"q75\"] = float(np.quantile(x, 0.75))\n",
    "    feats[\"iqr\"] = float(feats[\"q75\"] - feats[\"q25\"])\n",
    "    feats[\"skew\"] = float(skew(x))\n",
    "    feats[\"kurtosis\"] = float(kurtosis(x))\n",
    "\n",
    "    # ----- 신호 기반 전역 피처 -----\n",
    "    # zero crossing rate (전체 구간에서 비율)\n",
    "    sign = np.sign(x)\n",
    "    sign[sign == 0] = 1\n",
    "    zc = np.sum(sign[1:] != sign[:-1])\n",
    "    feats[\"zero_cross_rate\"] = float(zc / (len(x) - 1))\n",
    "\n",
    "    # peak 개수 (단순 1차 peak)\n",
    "    mid = np.arange(1, len(x) - 1)\n",
    "    cond = (x[mid] > x[mid - 1]) & (x[mid] > x[mid + 1])\n",
    "    feats[\"num_peaks\"] = float(np.sum(cond))\n",
    "\n",
    "    # ----- tsfresh 유사 전역 피처 -----\n",
    "    # abs_energy\n",
    "    feats[\"abs_energy\"] = float(np.sum(x ** 2))\n",
    "\n",
    "    # cid_ce (complexity-invariant distance 기반 간단 버전)\n",
    "    diff = np.diff(x)\n",
    "    feats[\"cid_ce\"] = float(np.sqrt(np.sum(diff ** 2)))\n",
    "\n",
    "    # ratio_beyond_2_sigma\n",
    "    mu = feats[\"mean\"]\n",
    "    sigma = feats[\"std\"] + 1e-12\n",
    "    feats[\"ratio_beyond_2_sigma\"] = float(\n",
    "        np.mean(np.abs(x - mu) > 2 * sigma)\n",
    "    )\n",
    "\n",
    "    # ----- spectral 피처 -----\n",
    "    spec_feats = compute_spectral_features(x)\n",
    "    feats.update(spec_feats)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ==================== 메인 루프 ====================\n",
    "\n",
    "lstm_sequences = []   # (N, seq_len, n_features)\n",
    "targets = []          # (N,)\n",
    "tf_feature_rows = []  # Transformer용 row(dict)의 리스트\n",
    "\n",
    "event_files = [f for f in os.listdir(BASE_DIR) if f.startswith(\"event_\") and f.endswith(\".csv\")]\n",
    "event_files = sorted(event_files)  # event_01, event_02, ...\n",
    "\n",
    "print(\"Found event files:\", event_files)\n",
    "\n",
    "for ev_file in event_files:\n",
    "    ev_path = os.path.join(BASE_DIR, ev_file)\n",
    "    print(f\"\\n▶ Processing {ev_file} ...\")\n",
    "    \n",
    "    df = pd.read_csv(ev_path)\n",
    "    # 예상 컬럼: acoustic_data, time_to_failure\n",
    "    signal = df[\"acoustic_data\"].values\n",
    "    ttf = df[\"time_to_failure\"].values\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    for seg_idx, seg_sig, seg_ttf in tqdm(\n",
    "        segment_iterator(signal, ttf, SEGMENT_SIZE),\n",
    "        desc=f\"Segments in {ev_file}\",\n",
    "        leave=False\n",
    "    ):\n",
    "        # ----- LSTM 시퀀스 -----\n",
    "        seq_features = build_lstm_sequence(seg_sig)\n",
    "        lstm_sequences.append(seq_features)\n",
    "\n",
    "        # ----- y (segment의 마지막 time_to_failure) -----\n",
    "        y_val = float(seg_ttf[-1])\n",
    "        targets.append(y_val)\n",
    "\n",
    "        # ----- Transformer summary features -----\n",
    "        tf_feats = compute_transformer_features(seg_sig)\n",
    "        tf_feats[\"event_file\"] = ev_file\n",
    "        tf_feats[\"segment_index\"] = int(seg_idx)\n",
    "        tf_feats[\"y\"] = y_val\n",
    "        tf_feature_rows.append(tf_feats)\n",
    "\n",
    "    # event별 메모리 정리\n",
    "    del signal, ttf\n",
    "    gc.collect()\n",
    "\n",
    "# ==================== 배열/데이터프레임로 변환 및 저장 ====================\n",
    "\n",
    "X_lstm = np.stack(lstm_sequences, axis=0)   # (N, seq_len, n_features)\n",
    "y = np.array(targets, dtype=np.float32)\n",
    "\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(\"X_lstm:\", X_lstm.shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "# 저장\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_lstm.npy\"), X_lstm)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"y.npy\"), y)\n",
    "\n",
    "tf_df = pd.DataFrame(tf_feature_rows)\n",
    "tf_path = os.path.join(OUTPUT_DIR, \"transformer_features.parquet\")\n",
    "tf_df.to_parquet(tf_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved:\")\n",
    "print(f\"- {os.path.join(OUTPUT_DIR, 'X_lstm.npy')}\")\n",
    "print(f\"- {os.path.join(OUTPUT_DIR, 'y.npy')}\")\n",
    "print(f\"- {tf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a13f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T06:34:02.907674Z",
     "iopub.status.busy": "2025-11-21T06:34:02.907373Z",
     "iopub.status.idle": "2025-11-21T06:35:12.927620Z",
     "shell.execute_reply": "2025-11-21T06:35:12.926511Z"
    },
    "papermill": {
     "duration": 70.104467,
     "end_time": "2025-11-21T06:35:12.929150",
     "exception": false,
     "start_time": "2025-11-21T06:34:02.824683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train size: 3765  Valid size: 419\n",
      "\n",
      "===== LSTM Training =====\n",
      "[LSTM][1] Train=3.5964 Valid=3.1531\n",
      "[LSTM][2] Train=3.0331 Valid=3.1694\n",
      "[LSTM][3] Train=3.0301 Valid=3.1691\n",
      "[LSTM][4] Train=3.0433 Valid=3.1199\n",
      "[LSTM][5] Train=2.9181 Valid=2.9034\n",
      "[LSTM][6] Train=2.7660 Valid=2.7305\n",
      "[LSTM][7] Train=2.6673 Valid=2.5717\n",
      "[LSTM][8] Train=2.8188 Valid=3.1058\n",
      "[LSTM][9] Train=3.0010 Valid=3.0855\n",
      "[LSTM][10] Train=2.9332 Valid=2.9890\n",
      "[LSTM][11] Train=2.8175 Valid=2.7736\n",
      "[LSTM][12] Train=2.6890 Valid=2.5755\n",
      "\n",
      "===== Transformer Training (clean) =====\n",
      "[TF][1] Train=3.2953 Valid=3.1514\n",
      "[TF][2] Train=3.0018 Valid=3.0329\n",
      "[TF][3] Train=2.8036 Valid=2.7142\n",
      "[TF][4] Train=2.6527 Valid=2.6737\n",
      "[TF][5] Train=2.6456 Valid=2.6129\n",
      "[TF][6] Train=2.5709 Valid=2.5603\n",
      "[TF][7] Train=2.5015 Valid=2.8208\n",
      "[TF][8] Train=2.4882 Valid=2.5298\n",
      "[TF][9] Train=2.4666 Valid=2.4064\n",
      "[TF][10] Train=2.4326 Valid=2.4123\n",
      "[TF][11] Train=2.3687 Valid=2.3880\n",
      "[TF][12] Train=2.3802 Valid=2.4532\n",
      "[TF][13] Train=2.3845 Valid=2.3543\n",
      "[TF][14] Train=2.3461 Valid=2.4898\n",
      "[TF][15] Train=2.3404 Valid=2.3385\n",
      "\n",
      "===== Validation MAE =====\n",
      "LSTM MAE       : 2.5860915\n",
      "Transformer MAE: 2.3398588\n",
      "Ensemble MAE   : 2.366201\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ================= 1. 데이터 로드 =================\n",
    "X_lstm = np.load(\"./processed_features/X_lstm.npy\")   # (N,1500,9)\n",
    "y = np.load(\"./processed_features/y.npy\")\n",
    "\n",
    "tf_df = pd.read_parquet(\"./processed_features/transformer_features.parquet\")\n",
    "drop_cols = [\"event_file\", \"segment_index\", \"y\"]\n",
    "feature_cols = [c for c in tf_df.columns if c not in drop_cols]\n",
    "\n",
    "X_tf_raw = tf_df[feature_cols].values.astype(\"float32\")\n",
    "y_tf = tf_df[\"y\"].values.astype(\"float32\")\n",
    "\n",
    "assert len(X_lstm) == len(X_tf_raw) == len(y)\n",
    "assert np.allclose(y, y_tf)\n",
    "\n",
    "indices = np.arange(len(y))\n",
    "train_idx, valid_idx = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "\n",
    "X_lstm_train = X_lstm[train_idx]\n",
    "X_lstm_valid = X_lstm[valid_idx]\n",
    "y_train = y[train_idx]\n",
    "y_valid = y[valid_idx]\n",
    "\n",
    "X_tf_train_raw = X_tf_raw[train_idx]\n",
    "X_tf_valid_raw = X_tf_raw[valid_idx]\n",
    "\n",
    "print(\"Train size:\", len(train_idx), \" Valid size:\", len(valid_idx))\n",
    "\n",
    "# --------- NaN/inf 처리 + 스케일링 (Transformer용) ---------\n",
    "X_tf_train_raw = np.nan_to_num(X_tf_train_raw, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_tf_valid_raw = np.nan_to_num(X_tf_valid_raw, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tf_train = scaler.fit_transform(X_tf_train_raw)\n",
    "X_tf_valid = scaler.transform(X_tf_valid_raw)\n",
    "\n",
    "X_tf_train = np.nan_to_num(X_tf_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_tf_valid = np.nan_to_num(X_tf_valid, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ================= 2. Dataset =================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, X_lstm, X_tf, y):\n",
    "        self.X_lstm = torch.tensor(X_lstm, dtype=torch.float32)\n",
    "        self.X_tf = torch.tensor(X_tf, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_lstm[idx], self.X_tf[idx], self.y[idx]\n",
    "\n",
    "train_ds = MultiModalDataset(X_lstm_train, X_tf_train, y_train)\n",
    "valid_ds = MultiModalDataset(X_lstm_valid, X_tf_valid, y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "# ================= 3. 모델 정의 =================\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, d_model=128, nhead=4,\n",
    "                 num_layers=2, dim_feedforward=256):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(1, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, F)\n",
    "        x = x.view(x.size(0), -1, 1)   # (B,F,1)\n",
    "        h = self.input_proj(x)         # (B,F,d_model)\n",
    "        h = self.encoder(h)            # (B,F,d_model)\n",
    "        h = h.mean(dim=1)              # (B,d_model)\n",
    "        out = self.fc(h)               # (B,1)\n",
    "        return out\n",
    "\n",
    "lstm_model = LSTMModel().to(device)\n",
    "tf_model = TabularTransformer(num_features=X_tf_train.shape[1]).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "opt_lstm = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
    "opt_tf = torch.optim.Adam(tf_model.parameters(), lr=5e-4)\n",
    "\n",
    "# ================= 4. LSTM 학습 =================\n",
    "print(\"\\n===== LSTM Training =====\")\n",
    "EPOCHS_LSTM = 12\n",
    "for epoch in range(EPOCHS_LSTM):\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb_lstm, xb_tf, yb in train_loader:\n",
    "        xb_lstm = xb_lstm.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        pred = lstm_model(xb_lstm)\n",
    "        loss = criterion(pred, yb)\n",
    "\n",
    "        opt_lstm.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_lstm.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb_lstm, xb_tf, yb in valid_loader:\n",
    "            xb_lstm = xb_lstm.to(device)\n",
    "            yb = yb.to(device)\n",
    "            pred = lstm_model(xb_lstm)\n",
    "            val_loss += criterion(pred, yb).item()\n",
    "\n",
    "    print(f\"[LSTM][{epoch+1}] Train={train_loss/len(train_loader):.4f} \"\n",
    "          f\"Valid={val_loss/len(valid_loader):.4f}\")\n",
    "\n",
    "# ================= 5. Transformer 학습 =================\n",
    "print(\"\\n===== Transformer Training (clean) =====\")\n",
    "EPOCHS_TF = 15\n",
    "for epoch in range(EPOCHS_TF):\n",
    "    tf_model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb_lstm, xb_tf, yb in train_loader:\n",
    "        xb_tf = xb_tf.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        pred = tf_model(xb_tf)\n",
    "        loss = criterion(pred, yb)\n",
    "\n",
    "        opt_tf.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(tf_model.parameters(), max_norm=5.0)\n",
    "        opt_tf.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    tf_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb_lstm, xb_tf, yb in valid_loader:\n",
    "            xb_tf = xb_tf.to(device)\n",
    "            yb = yb.to(device)\n",
    "            pred = tf_model(xb_tf)\n",
    "            val_loss += criterion(pred, yb).item()\n",
    "\n",
    "    print(f\"[TF][{epoch+1}] Train={train_loss/len(train_loader):.4f} \"\n",
    "          f\"Valid={val_loss/len(valid_loader):.4f}\")\n",
    "\n",
    "# ================= 6. 앙상블 MAE =================\n",
    "lstm_model.eval()\n",
    "tf_model.eval()\n",
    "\n",
    "y_true, p_lstm, p_tf, p_ens = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb_lstm, xb_tf, yb in valid_loader:\n",
    "        xb_lstm = xb_lstm.to(device)\n",
    "        xb_tf = xb_tf.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        out_lstm = lstm_model(xb_lstm)\n",
    "        out_tf = tf_model(xb_tf)\n",
    "        out_ens = 0.5 * out_lstm + 0.5 * out_tf\n",
    "\n",
    "        y_true.append(yb.cpu().numpy())\n",
    "        p_lstm.append(out_lstm.cpu().numpy())\n",
    "        p_tf.append(out_tf.cpu().numpy())\n",
    "        p_ens.append(out_ens.cpu().numpy())\n",
    "\n",
    "y_true = np.concatenate(y_true).ravel()\n",
    "p_lstm = np.concatenate(p_lstm).ravel()\n",
    "p_tf = np.concatenate(p_tf).ravel()\n",
    "p_ens = np.concatenate(p_ens).ravel()\n",
    "\n",
    "print(\"\\n===== Validation MAE =====\")\n",
    "print(\"LSTM MAE       :\", mean_absolute_error(y_true, p_lstm))\n",
    "print(\"Transformer MAE:\", mean_absolute_error(y_true, p_tf))\n",
    "print(\"Ensemble MAE   :\", mean_absolute_error(y_true, p_ens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45eaa1e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T06:35:13.126677Z",
     "iopub.status.busy": "2025-11-21T06:35:13.126262Z",
     "iopub.status.idle": "2025-11-21T06:35:13.153166Z",
     "shell.execute_reply": "2025-11-21T06:35:13.152176Z"
    },
    "papermill": {
     "duration": 0.11955,
     "end_time": "2025-11-21T06:35:13.154413",
     "exception": false,
     "start_time": "2025-11-21T06:35:13.034863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./results/metrics.json\n",
      "Saved: ./results/validation_predictions.csv\n",
      "Saved: ./results/final_report_summary.txt\n",
      "\n",
      "✅ Output 저장 완료. 이제 Save Version 눌러서 영구 저장하세요.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "\n",
    "# ============ 1) metrics.json 저장 ============\n",
    "metrics = {\n",
    "    \"lstm_mae\": float(mean_absolute_error(y_true, p_lstm)),\n",
    "    \"transformer_mae\": float(mean_absolute_error(y_true, p_tf)),\n",
    "    \"ensemble_mae\": float(mean_absolute_error(y_true, p_ens)),\n",
    "    \"n_valid\": int(len(y_true))\n",
    "}\n",
    "\n",
    "with open(\"./results/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Saved: ./results/metrics.json\")\n",
    "\n",
    "# ============ 2) validation_predictions.csv 저장 ============\n",
    "meta_valid = tf_df.iloc[valid_idx][[\"event_file\", \"segment_index\", \"y\"]].reset_index(drop=True)\n",
    "\n",
    "df_out = meta_valid.copy()\n",
    "df_out[\"y_true\"] = y_true\n",
    "df_out[\"y_pred_lstm\"] = p_lstm\n",
    "df_out[\"y_pred_tf\"] = p_tf\n",
    "df_out[\"y_pred_ensemble\"] = p_ens\n",
    "\n",
    "df_out.to_csv(\"./results/validation_predictions.csv\", index=False)\n",
    "print(\"Saved: ./results/validation_predictions.csv\")\n",
    "\n",
    "# ============ 3) final_report_summary.txt 저장 ============\n",
    "report_text = f\"\"\"\n",
    "[기계학습기초 프로젝트 결과 요약]\n",
    "\n",
    "Validation Set (n={len(y_true)})\n",
    "\n",
    "- LSTM MAE       : {metrics['lstm_mae']:.4f}\n",
    "- Transformer MAE: {metrics['transformer_mae']:.4f}\n",
    "- Ensemble MAE   : {metrics['ensemble_mae']:.4f}\n",
    "\n",
    "Ensemble(LSTM+TF)의 성능이 가장 우수함을 확인.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./results/final_report_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(\"Saved: ./results/final_report_summary.txt\")\n",
    "print(\"\\n✅ Output 저장 완료. 이제 Save Version 눌러서 영구 저장하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eff131",
   "metadata": {
    "papermill": {
     "duration": 0.094427,
     "end_time": "2025-11-21T06:35:13.351575",
     "exception": false,
     "start_time": "2025-11-21T06:35:13.257148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8783205,
     "sourceId": 13795582,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 533.718839,
   "end_time": "2025-11-21T06:35:16.095069",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-21T06:26:22.376230",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
