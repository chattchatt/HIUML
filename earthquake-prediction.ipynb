{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13749989,"sourceType":"datasetVersion","datasetId":8749159},{"sourceId":13816811,"sourceType":"datasetVersion","datasetId":8798522},{"sourceId":13817739,"sourceType":"datasetVersion","datasetId":8799255}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['NUMBA_CUDA_SUPPORTED'] = \"0\"\nos.environ[\"TSFRESH_NO_Numba\"] = \"1\"\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats, signal, fft\nfrom scipy.stats import skew, kurtosis\nimport librosa\nfrom tsfresh.feature_extraction import feature_calculators\nimport gc\nfrom tqdm.auto import tqdm\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\ninput_dir = '/kaggle/input/earthquake/split_events'\noutput_dir = '/kaggle/working/feature_created/'\nos.makedirs(output_dir, exist_ok=True)\n\nprint(\"Setup Complete.\")\n\n# --- 피처 함수 1: Basic Statistics ---\ndef calculate_basic_statistics(data):\n    features = {}\n    features['mean'] = np.mean(data); features['median'] = np.median(data)\n    features['min'] = np.min(data); features['max'] = np.max(data)\n    features['range'] = features['max'] - features['min']; features['std'] = np.std(data)\n    features['variance'] = np.var(data); features['q25'] = np.percentile(data, 25)\n    features['q75'] = np.percentile(data, 75); features['iqr'] = features['q75'] - features['q25']\n    features['skewness'] = skew(data); features['kurtosis'] = kurtosis(data)\n    return features\n\n# --- 피처 함수 2: Rolling Statistics ---\ndef calculate_rolling_statistics(data, window=1000):\n    features = {}\n    series = pd.Series(data)\n    rolling_mean = series.rolling(window=window, min_periods=1).mean()\n    features['rolling_mean_mean'] = rolling_mean.mean()\n    features['rolling_mean_std'] = rolling_mean.std(ddof=0)\n    rolling_std = series.rolling(window=window, min_periods=1).std()\n    features['rolling_std_mean'] = rolling_std.mean()\n    features['rolling_std_std'] = rolling_std.std(ddof=0)\n    return features\n\n# --- 피처 함수 3: Signal Features ---\ndef calculate_signal_features(data):\n    features = {}\n    features['zero_crossing_rate'] = len(np.where(np.diff(np.sign(data)))[0]) / len(data)\n    peaks, properties = signal.find_peaks(data, height=np.mean(data))\n    features['num_peaks'] = len(peaks)\n    features['peak_mean_height'] = np.mean(properties['peak_heights']) if len(peaks) > 0 else 0\n    return features\n\n# --- 피처 함수 4: Spectral Features ---\ndef calculate_spectral_features(data, sr=4000000, n_mfcc=13):\n    features = {}\n    data_float = data.astype(np.float32)\n    mfccs = librosa.feature.mfcc(y=data_float, sr=sr, n_mfcc=n_mfcc, n_mels=40)\n    for i in range(n_mfcc):\n        features[f'mfcc_{i}_mean'] = np.mean(mfccs[i])\n    fft_values = np.abs(fft.fft(data))\n    fft_freq = fft.fftfreq(len(data), 1/sr)\n    sorted_indices = np.argsort(fft_values)[::-1]\n    features['fft_magnitude_1'] = fft_values[sorted_indices[0]]\n    features['fft_frequency_1'] = np.abs(fft_freq[sorted_indices[0]])\n    return features\n\n# --- 피처 함수 5: Selected Tsfresh Features ---\ndef calculate_selected_tsfresh(x):\n    features = {}\n    features['tsf_abs_energy'] = feature_calculators.abs_energy(x)\n    features['tsf_mean_abs_change'] = feature_calculators.mean_abs_change(x)\n    features['tsf_binned_entropy'] = feature_calculators.binned_entropy(x, max_bins=10)\n    features['tsf_autocorrelation_lag10'] = feature_calculators.autocorrelation(x, lag=10)\n    features['tsf_longest_strike_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n    try:\n        features['tsf_ar_coefficient_1'] = feature_calculators.ar_coefficient(x, [{\"coeff\": 1, \"k\": 10}])[0][1]\n    except Exception:\n        features['tsf_ar_coefficient_1'] = np.nan\n    return features\n    try:\n        features['tsf_fft_real_coeff_1'] = feature_calculators.fft_coefficient(x, [{\"coeff\": 1, \"attr\": \"real\"}])[0][1]\n        features['tsf_fft_real_coeff_5'] = feature_calculators.fft_coefficient(x, [{\"coeff\": 5, \"attr\": \"real\"}])[0][1]\n    except Exception:\n        features['tsf_fft_real_coeff_1'], features['tsf_fft_real_coeff_5'] = np.nan, np.nan\n    try:\n        features['tsf_spkt_welch_density_5'] = feature_calculators.spkt_welch_density(x, [{\"coeff\": 5}])[0][1]\n    except Exception:\n        features['tsf_spkt_welch_density_5'] = np.nan\n    try:\n        # CWT: 특정 스케일(주파수 대역)에서의 에너지\n        cwt_coeffs = feature_calculators.cwt_coefficients(x, widths=[2, 5, 10, 20], coeff=5, w=5)\n        features['tsf_cwt_coeff_5_w5'] = cwt_coeffs[0][1] if cwt_coeffs else np.nan\n    except Exception:\n        features['tsf_cwt_coeff_5_w5'] = np.nan\n    try:\n        features['tsf_approximate_entropy'] = feature_calculators.approximate_entropy(x, m=2, r=0.5)\n    except Exception:\n        features['tsf_approximate_entropy'] = np.nan\n\ndef extract_comprehensive_features(segment_data, target):\n    data = segment_data.values\n    \n    features = calculate_basic_statistics(data)\n    features.update(calculate_rolling_statistics(data))\n    features.update(calculate_signal_features(data))\n    features.update(calculate_spectral_features(data))\n    features.update(calculate_selected_tsfresh(data))\n    \n    features['time_to_failure'] = target\n    return features\n\ndef segment_generator(cycle_df):\n    n_samples = len(cycle_df)\n    segment_len = 150000\n    for i in range(n_samples, segment_len -1, -segment_len):\n        start = i - segment_len; end = i\n        segment_df = cycle_df.iloc[start:end]\n        yield segment_df['acoustic_data'], segment_df['time_to_failure'].iloc[-1]\n    rem_len = n_samples % segment_len\n    if rem_len > 0:\n        leftover_df = cycle_df.iloc[:rem_len]\n        padding_value = leftover_df['acoustic_data'].median()\n        padding = pd.Series([padding_value] * (segment_len - rem_len), dtype=np.int16)\n        padded_segment = pd.concat([padding, leftover_df['acoustic_data']], ignore_index=True)\n        yield padded_segment, leftover_df['time_to_failure'].iloc[-1]\n\nstart_total_time = time.time()\nprint(\"\\nStarting comprehensive feature extraction for all 17 event files...\")\n\nfor cycle_id in range(1, 18):\n    start_cycle_time = time.time()\n    input_file = os.path.join(input_dir, f'event_{cycle_id:02d}.csv')\n    \n    if not os.path.exists(input_file):\n        print(f\"Warning: File not found for event {cycle_id}, skipping. Path: {input_file}\")\n        continue\n        \n    print(f\"\\n--- Processing Event {cycle_id}/{17} ---\")\n\n    try:\n        cycle_df = pd.read_csv(input_file)\n        print(f\"  Data loaded ({len(cycle_df):,} rows).\")\n\n        feature_rows = []\n        num_segments = (len(cycle_df) + 149999) // 150000\n        for seg_data, target in tqdm(segment_generator(cycle_df), total=num_segments, desc=f\"  Event {cycle_id} Segments\"):\n            features = extract_comprehensive_features(seg_data, target)\n            features['cycle_id'] = cycle_id\n            feature_rows.append(features)\n        \n        del cycle_df\n        gc.collect()\n\n        cycle_feature_df = pd.DataFrame(feature_rows)\n        cycle_feature_df = cycle_feature_df.iloc[::-1].reset_index(drop=True)\n        cycle_feature_df['segment_in_cycle_id'] = range(len(cycle_feature_df))\n        \n        output_file = os.path.join(output_dir, f'event_{cycle_id:02d}.parquet')\n        cycle_feature_df.to_parquet(output_file, index=False)\n\n        end_cycle_time = time.time()\n        print(f\"  Finished processing Event {cycle_id}, generated {len(feature_rows)} segments.\")\n        print(f\"  Time taken for this event: {(end_cycle_time - start_cycle_time):.2f} seconds.\")\n        print(f\"  Feature file saved to: {output_file}\")\n\n    except Exception as e:\n        print(f\"An error occurred while processing Event {cycle_id}: {e}\")\n\nend_total_time = time.time()\nprint(f\"All 17 feature files are saved in the directory: {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom tqdm.auto import tqdm\n\ninput_dir = '/kaggle/working/feature_created/' \noutput_dir = '/kaggle/working/'\nfinal_output_file = os.path.join(output_dir, 'final_features_comprehensive.parquet')\n\nall_dfs_list = []\nprint(\"Starting to combine all 17 feature files...\")\n\nfor i in tqdm(range(1, 18), desc=\"Combining Files\"):\n    file_path = os.path.join(input_dir, f'event_{i:02d}.parquet')\n    try:\n        df_part = pd.read_parquet(file_path)\n        all_dfs_list.append(df_part)\n    except FileNotFoundError:\n        print(f\"Warning: File not found for event {i}, skipping.\")\n\nif all_dfs_list:\n    final_df = pd.concat(all_dfs_list, ignore_index=True)\n    print(f\"\\nSuccessfully combined all parts. Total rows: {len(final_df)}\")\n    \n    print(f\"Saving final combined data to Parquet file: {final_output_file}\")\n    final_df.to_parquet(final_output_file, index=False)\n    print(\"Final file saved successfully.\")\nelse:\n    print(\"No intermediate files found to combine.\")\n\nprint(\"\\n--- Final DataFrame Info ---\")\nfinal_df.info()\nprint(\"\\n--- First 5 rows ---\")\nprint(final_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfeature_file = '/kaggle/working/final_features_comprehensive.parquet'\ndf = pd.read_parquet(feature_file)\ndf = df.fillna(0)\n\nX = df.drop(columns=['time_to_failure', 'cycle_id', 'segment_in_cycle_id'])\ny = df['time_to_failure']\ngroups = df['cycle_id']\n\nn_splits = 5\ngkf = GroupKFold(n_splits=n_splits)\n\nfeature_importances = pd.DataFrame(index=X.columns)\n\nprint(f\"Starting {n_splits}-Fold GroupKFold to calculate feature importances...\")\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n    print(f\"--- Fold {fold+1}/{n_splits} ---\")\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    \n    model = lgb.LGBMRegressor(\n        objective='mae',\n        n_estimators=1000,\n        learning_rate=0.05,\n        n_jobs=-1,\n        random_state=42\n    )\n    \n    model.fit(X_train, y_train,\n              eval_set=[(X_val, y_val)],\n              eval_metric='mae',\n              callbacks=[lgb.early_stopping(100, verbose=False)])\n    \n    feature_importances[f'fold_{fold+1}'] = model.feature_importances_\n\nfeature_importances['average'] = feature_importances.mean(axis=1)\nfeature_importances = feature_importances.sort_values(by='average', ascending=False)\n\nplt.figure(figsize=(12, 10))\nsns.barplot(x='average', y=feature_importances.head(30).index, data=feature_importances.head(30))\nplt.title('Top 30 Feature Importances (Averaged over GroupKFold)')\nplt.xlabel('Average Importance')\nplt.ylabel('Features')\nplt.tight_layout()\nplt.show()\n\ntop_30_features = feature_importances.head(30).index.tolist()\nprint(\"\\n--- Top 30 Most Important Features ---\")\nprint(top_30_features)\n\ntop_features_df = df[top_30_features]\ncorrelation_matrix = top_features_df.corr().abs()\n\nupper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n\nprint(f\"\\nFound {len(to_drop)} features to drop due to high correlation (>0.95):\")\nprint(to_drop)\n\nfinal_feature_list = [f for f in top_30_features if f not in to_drop]\n\nprint(f\"\\n--- Final Selected Features ({len(final_feature_list)} features) ---\")\nprint(final_feature_list)\n\ndf_final_train = pd.read_parquet('/kaggle/working/final_features_comprehensive.parquet')\n\nX_final = df_final_train[final_feature_list]\ny_final = df_final_train['time_to_failure']\ngroups_final = df_final_train['cycle_id']\n\nprint(f\"Shape of X_final: {X_final.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nfeature_file = '/kaggle/input/feature-created/final_features_comprehensive.parquet'\ndf_final_train = pd.read_parquet(feature_file).fillna(0)\nfinal_feature_list = [\n    'mfcc_5_mean', 'mfcc_11_mean', 'mfcc_10_mean', 'skewness', 'tsf_binned_entropy', \n    'mfcc_9_mean', 'mfcc_7_mean', 'mfcc_6_mean', 'peak_mean_height', 'mean', \n    'zero_crossing_rate', 'mfcc_8_mean', 'mfcc_4_mean', 'mfcc_12_mean', 'rolling_mean_std', \n    'mfcc_0_mean', 'num_peaks', 'tsf_autocorrelation_lag10', 'mfcc_3_mean', 'mfcc_2_mean', \n    'kurtosis', 'rolling_std_std', 'tsf_mean_abs_change', 'tsf_ar_coefficient_1', \n    'tsf_longest_strike_above_mean'\n]\nX = df_final_train[final_feature_list]\ny = df_final_train['time_to_failure']\ngroups = df_final_train['cycle_id']\n\nlgb_params = {\n    'objective': 'mae', \n    'metric': 'mae', \n    'n_estimators': 10000, \n    'learning_rate': 0.01,\n    'feature_fraction': 0.7, \n    'bagging_fraction': 0.7, \n    'bagging_freq': 1,\n    'lambda_l1': 0.3, \n    'lambda_l2': 0.3, \n    'num_leaves': 31, \n    'verbose': -1,\n    'n_jobs': -1, \n    'seed': 42, \n    'boosting_type': 'gbdt',\n}\n\ndef transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n    x = layers.Dropout(dropout)(x)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    res = x + inputs\n    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    return x + res\n\ndef build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n    inputs = keras.Input(shape=input_shape)\n    x = inputs\n    for _ in range(num_transformer_blocks):\n        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n    for dim in mlp_units:\n        x = layers.Dense(dim, activation=\"relu\")(x)\n        x = layers.Dropout(mlp_dropout)(x)\n    outputs = layers.Dense(1)(x)\n    model = keras.Model(inputs, outputs)\n    model.compile(loss=\"mae\", optimizer=keras.optimizers.Adam(learning_rate=1e-3), metrics=[\"mae\"])\n    return model\n\nn_splits = 5\ngkf = GroupKFold(n_splits=n_splits)\n\noof_lgb = np.zeros(len(X))\noof_transformer = np.zeros(len(X))\nlgb_models = []\ntransformer_models = []\nscalers = []\n\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n    print(f\"===================== FOLD {fold+1}/{n_splits} =====================\")\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # --- LightGBM 학습 및 저장 ---\n    print(\"--- Training LightGBM model... ---\")\n    model_lgb = lgb.LGBMRegressor(**lgb_params)\n    model_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae', callbacks=[lgb.early_stopping(200, verbose=False)])\n    oof_lgb[val_idx] = model_lgb.predict(X_val)\n    lgb_models.append(model_lgb)\n    \n    # --- Transformer 학습 및 저장 ---\n    print(\"--- Training Transformer model... ---\")\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    scalers.append(scaler)\n    \n    X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n    X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n    \n    model_transformer = build_transformer_model(\n        input_shape=X_train_reshaped.shape[1:], head_size=256, num_heads=4, ff_dim=4,\n        num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.45, dropout=0.3,\n    )\n    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=20, mode=\"min\", restore_best_weights=True)\n    model_transformer.fit(X_train_reshaped, y_train, epochs=200, batch_size=64, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping], verbose=0)\n    oof_transformer[val_idx] = model_transformer.predict(X_val_reshaped).flatten()\n    transformer_models.append(model_transformer) # 학습된 모델을 리스트에 저장\n\n    print(f\"Fold {fold+1} MAE (LGBM): {mean_absolute_error(y_val, oof_lgb[val_idx]):.4f}\")\n    print(f\"Fold {fold+1} MAE (Transformer): {mean_absolute_error(y_val, oof_transformer[val_idx]):.4f}\")\n    gc.collect()\n\noof_ensemble = 0.5 * oof_lgb + 0.5 * oof_transformer\nprint(f\"Overall OOF MAE (LGBM): {mean_absolute_error(y, oof_lgb):.4f}\")\nprint(f\"Overall OOF MAE (Transformer): {mean_absolute_error(y, oof_transformer):.4f}\")\nprint(f\"Overall OOF MAE (Ensemble 0.5/0.5): {mean_absolute_error(y, oof_ensemble):.4f}\")\n\nprint(f\"\\nTraining complete. {len(lgb_models)} LGBM models, {len(transformer_models)} Transformer models, and {len(scalers)} scalers are saved in memory.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport gc\nfrom scipy import stats, signal, fft\nfrom scipy.stats import skew, kurtosis\nimport librosa\nfrom tsfresh.feature_extraction import feature_calculators\nfrom sklearn.metrics import mean_absolute_error\nimport time\nimport warnings\nimport os\n\nbest_w = 0\nmin_mae = 100\nfor w in np.arange(0, 1.01, 0.01):\n    oof_ens = w * oof_lgb + (1 - w) * oof_transformer\n    mae = mean_absolute_error(y, oof_ens)\n    if mae < min_mae:\n        min_mae = mae\n        best_w = w\nprint(f\"Optimal weight for LGBM: {best_w:.2f}\")\nprint(f\"Optimal weight for Transformer: {1-best_w:.2f}\")\nprint(f\"Best possible OOF MAE with these models: {min_mae:.4f}\")\n\nsubmission = pd.read_csv('/kaggle/input/testing/sample_submission.csv', index_col='seg_id')\ntest_files = submission.index.tolist()\n\npredictions = []\n\ndef extract_comprehensive_features(segment_data, target):\n    data = segment_data.values\n    \n    features = calculate_basic_statistics(data)\n    features.update(calculate_rolling_statistics(data))\n    features.update(calculate_signal_features(data))\n    features.update(calculate_spectral_features(data))\n    features.update(calculate_selected_tsfresh(data))\n    \n    features['time_to_failure'] = target\n    return features\n\ndef calculate_basic_statistics(data):\n    features = {}\n    features['mean'] = np.mean(data); features['median'] = np.median(data)\n    features['min'] = np.min(data); features['max'] = np.max(data)\n    features['range'] = features['max'] - features['min']; features['std'] = np.std(data)\n    features['variance'] = np.var(data); features['q25'] = np.percentile(data, 25)\n    features['q75'] = np.percentile(data, 75); features['iqr'] = features['q75'] - features['q25']\n    features['skewness'] = skew(data); features['kurtosis'] = kurtosis(data)\n    return features\n\ndef calculate_rolling_statistics(data, window=1000):\n    features = {}\n    series = pd.Series(data)\n    rolling_mean = series.rolling(window=window, min_periods=1).mean()\n    features['rolling_mean_mean'] = rolling_mean.mean()\n    features['rolling_mean_std'] = rolling_mean.std(ddof=0)\n    rolling_std = series.rolling(window=window, min_periods=1).std()\n    features['rolling_std_mean'] = rolling_std.mean()\n    features['rolling_std_std'] = rolling_std.std(ddof=0)\n    return features\n\ndef calculate_signal_features(data):\n    features = {}\n    features['zero_crossing_rate'] = len(np.where(np.diff(np.sign(data)))[0]) / len(data)\n    peaks, properties = signal.find_peaks(data, height=np.mean(data))\n    features['num_peaks'] = len(peaks)\n    features['peak_mean_height'] = np.mean(properties['peak_heights']) if len(peaks) > 0 else 0\n    return features\n\ndef calculate_spectral_features(data, sr=4000000, n_mfcc=13):\n    features = {}\n    data_float = data.astype(np.float32)\n    mfccs = librosa.feature.mfcc(y=data_float, sr=sr, n_mfcc=n_mfcc, n_mels=40)\n    for i in range(n_mfcc):\n        features[f'mfcc_{i}_mean'] = np.mean(mfccs[i])\n    fft_values = np.abs(fft.fft(data))\n    fft_freq = fft.fftfreq(len(data), 1/sr)\n    sorted_indices = np.argsort(fft_values)[::-1]\n    features['fft_magnitude_1'] = fft_values[sorted_indices[0]]\n    features['fft_frequency_1'] = np.abs(fft_freq[sorted_indices[0]])\n    return features\n\ndef calculate_selected_tsfresh(x):\n    features = {}\n    features['tsf_abs_energy'] = feature_calculators.abs_energy(x)\n    features['tsf_mean_abs_change'] = feature_calculators.mean_abs_change(x)\n    features['tsf_binned_entropy'] = feature_calculators.binned_entropy(x, max_bins=10)\n    features['tsf_autocorrelation_lag10'] = feature_calculators.autocorrelation(x, lag=10)\n    features['tsf_longest_strike_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n    try:\n        features['tsf_ar_coefficient_1'] = feature_calculators.ar_coefficient(x, [{\"coeff\": 1, \"k\": 10}])[0][1]\n    except Exception:\n        features['tsf_ar_coefficient_1'] = np.nan\n    return features\n    try:\n        features['tsf_fft_real_coeff_1'] = feature_calculators.fft_coefficient(x, [{\"coeff\": 1, \"attr\": \"real\"}])[0][1]\n        features['tsf_fft_real_coeff_5'] = feature_calculators.fft_coefficient(x, [{\"coeff\": 5, \"attr\": \"real\"}])[0][1]\n    except Exception:\n        features['tsf_fft_real_coeff_1'], features['tsf_fft_real_coeff_5'] = np.nan, np.nan\n    try:\n        features['tsf_spkt_welch_density_5'] = feature_calculators.spkt_welch_density(x, [{\"coeff\": 5}])[0][1]\n    except Exception:\n        features['tsf_spkt_welch_density_5'] = np.nan\n    try:\n        # CWT: 특정 스케일(주파수 대역)에서의 에너지\n        cwt_coeffs = feature_calculators.cwt_coefficients(x, widths=[2, 5, 10, 20], coeff=5, w=5)\n        features['tsf_cwt_coeff_5_w5'] = cwt_coeffs[0][1] if cwt_coeffs else np.nan\n    except Exception:\n        features['tsf_cwt_coeff_5_w5'] = np.nan\n    try:\n        features['tsf_approximate_entropy'] = feature_calculators.approximate_entropy(x, m=2, r=0.5)\n    except Exception:\n        features['tsf_approximate_entropy'] = np.nan\n\ndef segment_generator(cycle_df):\n    n_samples = len(cycle_df)\n    segment_len = 150000\n    for i in range(n_samples, segment_len -1, -segment_len):\n        start = i - segment_len; end = i\n        segment_df = cycle_df.iloc[start:end]\n        yield segment_df['acoustic_data'], segment_df['time_to_failure'].iloc[-1]\n    rem_len = n_samples % segment_len\n    if rem_len > 0:\n        leftover_df = cycle_df.iloc[:rem_len]\n        padding_value = leftover_df['acoustic_data'].median()\n        padding = pd.Series([padding_value] * (segment_len - rem_len), dtype=np.int16)\n        padded_segment = pd.concat([padding, leftover_df['acoustic_data']], ignore_index=True)\n        yield padded_segment, leftover_df['time_to_failure'].iloc[-1]\n\nfor seg_id in tqdm(test_files, desc=\"Predicting on test segments\"):\n    test_df = pd.read_csv(f'/kaggle/input/testing/test/{seg_id}.csv')\n    \n    test_features = extract_comprehensive_features(test_df['acoustic_data'], target=0) \n    X_test = pd.DataFrame([test_features])[final_feature_list]\n\n    fold_preds_lgb = []\n    fold_preds_transformer = []\n\n    for i in range(n_splits):\n        model_lgb = lgb_models[i]\n        model_transformer = transformer_models[i]\n        scaler = scalers[i]\n\n        pred_lgb = model_lgb.predict(X_test)[0]\n        fold_preds_lgb.append(pred_lgb)\n\n        X_test_scaled = scaler.transform(X_test)\n        X_test_reshaped = X_test_scaled.reshape((1, 1, X_test.shape[1]))\n        pred_transformer = model_transformer.predict(X_test_reshaped, verbose=0)[0][0]\n        fold_preds_transformer.append(pred_transformer)\n\n    avg_pred_lgb = np.mean(fold_preds_lgb)\n    avg_pred_transformer = np.mean(fold_preds_transformer)\n\n    final_prediction = best_w * avg_pred_lgb + (1 - best_w) * avg_pred_transformer\n    predictions.append(final_prediction)\n    gc.collect()\n\nsubmission['time_to_failure'] = predictions\nsubmission.to_csv('submission.csv')\n\nprint(\"\\nSubmission file created!\")\nprint(\"-First 5 predictions-\")\nprint(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}